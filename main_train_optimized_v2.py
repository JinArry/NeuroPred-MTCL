import os
import random
import numpy as np
import tensorflow as tf
import json
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from model06_v2 import New_Model_06_Attn

# ç»å¯¹è·¯å¾„åŸºå‡†ç›®å½•ï¼ˆå½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ï¼‰
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
FEATURES_DIR = os.path.join(BASE_DIR, "features")
CHECKPOINTS_DIR = os.path.join(BASE_DIR, "checkpoints")

os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # åˆ‡æ¢åˆ°GPU 0ï¼Œé¿å…å†…å­˜å†²çª


def set_global_determinism(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
    print(f"éšæœºç§å­å·²è®¾ç½®ä¸ºï¼š{seed}")


x_train = np.load(os.path.join(FEATURES_DIR, "train_esm_seq.npy"))
y_train = np.load(os.path.join(FEATURES_DIR, "train_labels_seq.npy"))
x_val = np.load(os.path.join(FEATURES_DIR, "val_esm_seq.npy"))
y_val = np.load(os.path.join(FEATURES_DIR, "val_labels_seq.npy"))

# ç‹¬çƒ­ç¼–ç 
num_classes = len(np.unique(y_train))
y_train_cat = to_categorical(y_train, num_classes)
y_val_cat = to_categorical(y_val, num_classes)

params = {
    
    
    # random search
    "hidden_dim": 160,
    "projection_dim": 96,
    "alpha": 0.6, # çŸ¥è¯†è’¸é¦æƒé‡0.6
    "beta": 0.03, # å¯¹æ¯”å­¦ä¹ æƒé‡0.03
    "temperature": 0.1,  # æ¨¡å‹é‡Œè¦æ¥æ”¶å¹¶ä½¿ç”¨
    "learning_rate": 0.001,
    "batch_size": 64,
    "num_heads": 8,
    "dropout_rate": 0.0,  # å®Œå…¨ç§»é™¤dropoutï¼Œå›åˆ°åŸå§‹çŠ¶æ€ï¼ˆåŸå§‹æ¨¡å‹æ— dropoutæ—¶æ€§èƒ½æœ€å¥½0.936ï¼‰
    # "label_smoothing": 0.05,  # æ ‡ç­¾å¹³æ»‘ç³»æ•°ï¼ˆç»è¿‡æµ‹è¯•ï¼š0.05æœ€ä¼˜ACC=0.928ï¼Œ0.1=0.921ï¼Œ0.2=0.914ï¼‰
    # "grad_clip_norm": 5.0  # æ¢¯åº¦è£å‰ªï¼ˆç»è¿‡æµ‹è¯•ï¼š5.0æœ€ä¼˜ACC=0.931ï¼Œ4.0=0.929ï¼Œ1.0=0.927ï¼Œ2.0=0.926ï¼Œ3.0=0.917ï¼‰
}

# for seed in range(30, 40):
for seed in range(37, 38):
    # å›ºå®šéšæœºç§å­
    set_global_determinism(seed)

    batch = params["batch_size"]
    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train_cat))
    train_ds = train_ds.shuffle(1024).batch(batch).prefetch(tf.data.AUTOTUNE)
    #train_ds = train_ds.shuffle(1024, seed=seed, reshuffle_each_iteration=True).batch(batch).prefetch(tf.data.AUTOTUNE)

    val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val_cat))
    val_ds = val_ds.batch(batch).prefetch(tf.data.AUTOTUNE)

    model = New_Model_06_Attn(
        hidden_dim=params["hidden_dim"],
        projection_dim=params["projection_dim"],
        num_heads=params["num_heads"],
        alpha=params["alpha"],
        beta=params["beta"],
        dropout_rate=params.get("dropout_rate", 0.1),  # æ·»åŠ dropout_rateå‚æ•°
        label_smoothing=params.get("label_smoothing", 0.0),  # æ ‡ç­¾å¹³æ»‘ç³»æ•°ï¼ˆå·²ç¦ç”¨ï¼Œè®¾ä¸º0.0ï¼‰
        grad_clip_norm=params.get("grad_clip_norm", None),  # æ¢¯åº¦è£å‰ªï¼ˆå·²ç¦ç”¨ï¼Œè®¾ä¸ºNoneï¼‰
    )

    optimizer = Adam(learning_rate=params["learning_rate"])
    model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)
    ckpt_h5 = os.path.join(
        CHECKPOINTS_DIR,
        f"Model06_v2_Attn_seed{seed}_best.weights.h5"
    )
    checkpoint_cb = ModelCheckpoint(
        filepath=ckpt_h5,
        monitor='val_accuracy',  # ç›¯éªŒè¯é›†å‡†ç¡®ç‡
        mode='max',
        save_best_only=True,  # åªåœ¨å‡ºç°æ›´ä¼˜æˆç»©æ—¶ä¿å­˜ï¼ˆè¦†ç›–ï¼‰
        save_weights_only=True,  # ä¿å­˜ä¸º .h5 æƒé‡ï¼Œæœ€ç¨³
        verbose=1
    )
    
    # æ—©åœæœºåˆ¶ - é˜²æ­¢è¿‡æ‹Ÿåˆ
    early_stop_cb = EarlyStopping(
        monitor='val_accuracy',
        patience=8,  # 8ä¸ªepochæ²¡æœ‰æå‡å°±åœæ­¢ï¼Œä»8->5
        restore_best_weights=True,  # æ¢å¤æœ€ä½³æƒé‡
        verbose=1
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦ - è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡
    reduce_lr_cb = ReduceLROnPlateau(
        monitor='val_accuracy',
        factor=0.5,  # å­¦ä¹ ç‡å‡åŠ
        patience=3,  # 3ä¸ªepochæ²¡æœ‰æå‡å°±é™ä½å­¦ä¹ ç‡
        min_lr=1e-6,  # æœ€å°å­¦ä¹ ç‡
        verbose=1
    )

    # ä½¿ç”¨ dataset è¿›è¡Œè®­ç»ƒ
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=50,  # è®¾ç½®è¾ƒå¤§å€¼ï¼Œè®©æ—©åœå†³å®šä½•æ—¶åœæ­¢
        callbacks=[checkpoint_cb, early_stop_cb, reduce_lr_cb]
    )
    val_acc_hist = history.history.get('val_accuracy', [])
    train_acc_hist = history.history.get('accuracy', [])
    train_loss_hist = history.history.get('loss', [])
    val_loss_hist = history.history.get('val_loss', [])
    
    if len(val_acc_hist) > 0:
        best_epoch = int(np.argmax(val_acc_hist) + 1)  # 1-based
        best_val = float(np.max(val_acc_hist))
        final_val = float(val_acc_hist[-1])
        
        # ========== è®­ç»ƒè¯Šæ–­æŠ¥å‘Š ==========
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Seed {seed} è®­ç»ƒè¯Šæ–­æŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"æ€»è®­ç»ƒè½®æ•°: {len(val_acc_hist)}")
        print(f"æœ€ä½³epoch: {best_epoch}")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val:.4f}")
        print(f"æœ€åéªŒè¯å‡†ç¡®ç‡: {final_val:.4f}")
        print(f"æ€§èƒ½å˜åŒ–: {final_val - best_val:+.4f}")
        
        # æ£€æŸ¥æ˜¯å¦æå‰æ”¶æ•›
        if best_epoch < len(val_acc_hist) * 0.7:
            print(f"\nâš ï¸  æ¨¡å‹åœ¨{best_epoch}ä¸ªepochå°±æ”¶æ•›äº†ï¼ˆå æ€»è½®æ•°çš„{best_epoch/len(val_acc_hist)*100:.1f}%ï¼‰")
            print(f"   å»ºè®®: æ·»åŠ æ—©åœæœºåˆ¶ï¼Œè®¾ç½®epochs=50+ï¼Œè®©æ—©åœå†³å®šä½•æ—¶åœæ­¢")
        elif best_epoch == len(val_acc_hist):
            print(f"\nâœ… æ¨¡å‹åœ¨æœ€åepochè¾¾åˆ°æœ€ä½³ï¼Œå¯èƒ½éœ€è¦æ›´å¤šepochsæˆ–å­¦ä¹ ç‡è°ƒåº¦")
        else:
            print(f"\nâš ï¸  æ¨¡å‹åœ¨{best_epoch}ä¸ªepochåæ€§èƒ½ä¸‹é™ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")
            print(f"   å»ºè®®: æ·»åŠ æ—©åœæœºåˆ¶ï¼Œæ¢å¤æœ€ä½³æƒé‡")
        
        # æ£€æŸ¥è¿‡æ‹Ÿåˆæƒ…å†µ
        if len(train_acc_hist) > 0:
            best_train_acc = float(train_acc_hist[best_epoch - 1])
            final_train_acc = float(train_acc_hist[-1])
            best_gap = best_train_acc - best_val
            final_gap = final_train_acc - final_val
            
            print(f"\nğŸ“ˆ è¿‡æ‹Ÿåˆåˆ†æ:")
            print(f"   æœ€ä½³epochè®­ç»ƒå‡†ç¡®ç‡: {best_train_acc:.4f}")
            print(f"   æœ€ä½³epochéªŒè¯å‡†ç¡®ç‡: {best_val:.4f}")
            print(f"   æœ€ä½³epochæ—¶è®­ç»ƒ-éªŒè¯å·®è·: {best_gap:.4f}")
            print(f"   æœ€åepochè®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.4f}")
            print(f"   æœ€åepochéªŒè¯å‡†ç¡®ç‡: {final_val:.4f}")
            print(f"   æœ€åepochæ—¶è®­ç»ƒ-éªŒè¯å·®è·: {final_gap:.4f}")
            
            if final_gap > best_gap + 0.05:  # å·®è·å¢åŠ è¶…è¿‡5%
                print(f"\nâš ï¸  å­˜åœ¨è¿‡æ‹Ÿåˆè¶‹åŠ¿ï¼ˆå·®è·ä»{best_gap:.4f}å¢åŠ åˆ°{final_gap:.4f}ï¼‰")
                print(f"   å»ºè®®: æ·»åŠ Dropoutæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ")
            elif final_gap > 0.1:
                print(f"\nâš ï¸  è®­ç»ƒ-éªŒè¯å·®è·è¾ƒå¤§: {final_gap:.4f}")
                print(f"   å»ºè®®: è€ƒè™‘æ·»åŠ æ­£åˆ™åŒ–")
            else:
                print(f"\nâœ… è®­ç»ƒ-éªŒè¯å·®è·åˆç†ï¼Œæœªå‘ç°æ˜æ˜¾è¿‡æ‹Ÿåˆ")
        
        # Losså˜åŒ–åˆ†æ
        if len(train_loss_hist) > 0 and len(val_loss_hist) > 0:
            initial_train_loss = float(train_loss_hist[0])
            final_train_loss = float(train_loss_hist[-1])
            best_val_loss = float(min(val_loss_hist))
            best_val_loss_epoch = int(np.argmin(val_loss_hist) + 1)
            final_val_loss = float(val_loss_hist[-1])
            train_loss_reduction = ((initial_train_loss - final_train_loss) / initial_train_loss) * 100
            
            print(f"\nğŸ“‰ Losså˜åŒ–åˆ†æ:")
            print(f"   åˆå§‹è®­ç»ƒLoss: {initial_train_loss:.4f}")
            print(f"   æœ€ç»ˆè®­ç»ƒLoss: {final_train_loss:.4f}")
            print(f"   è®­ç»ƒLossä¸‹é™: {train_loss_reduction:.2f}%")
            print(f"   æœ€ä½³éªŒè¯Loss: {best_val_loss:.4f} (Epoch {best_val_loss_epoch})")
            print(f"   æœ€ç»ˆéªŒè¯Loss: {final_val_loss:.4f}")
            print(f"   éªŒè¯Losså˜åŒ–: {final_val_loss - best_val_loss:+.4f}")
            
            # åˆ†æLossè¶‹åŠ¿
            if final_val_loss > best_val_loss + 0.05:
                print(f"   âš ï¸  éªŒè¯Lossåœ¨æœ€ä½³epochåä¸Šå‡ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ")
            elif final_val_loss < best_val_loss:
                print(f"   âœ… éªŒè¯LossæŒç»­ä¸‹é™ï¼Œè®­ç»ƒè‰¯å¥½")
            else:
                print(f"   âœ… éªŒè¯LossåŸºæœ¬ç¨³å®š")
            
            # æ£€æŸ¥è®­ç»ƒ-éªŒè¯Losså·®è·
            best_train_loss_at_best_epoch = float(train_loss_hist[best_val_loss_epoch - 1])
            loss_gap_at_best = best_train_loss_at_best_epoch - best_val_loss
            loss_gap_final = final_train_loss - final_val_loss
            
            print(f"\n   è®­ç»ƒ-éªŒè¯Losså·®è·:")
            print(f"   æœ€ä½³epochæ—¶: {loss_gap_at_best:+.4f}")
            print(f"   æœ€ç»ˆepochæ—¶: {loss_gap_final:+.4f}")
            
            if loss_gap_final > loss_gap_at_best + 0.05:
                print(f"   âš ï¸  Losså·®è·å¢å¤§ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆè¶‹åŠ¿")
            elif abs(loss_gap_final) < 0.1:
                print(f"   âœ… Losså·®è·åˆç†ï¼Œæ¨¡å‹æ³›åŒ–è‰¯å¥½")
        
        # æ£€æŸ¥å­¦ä¹ ç‡å˜åŒ–
        if 'lr' in history.history:
            lr_hist = history.history['lr']
            print(f"\nğŸ“‰ å­¦ä¹ ç‡åˆ†æ:")
            print(f"   åˆå§‹å­¦ä¹ ç‡: {lr_hist[0]:.6f}")
            print(f"   æœ€ç»ˆå­¦ä¹ ç‡: {lr_hist[-1]:.6f}")
            if lr_hist[0] == lr_hist[-1]:
                print(f"   âš ï¸  å­¦ä¹ ç‡æ²¡æœ‰å˜åŒ–ï¼Œå»ºè®®æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦ï¼ˆReduceLROnPlateauï¼‰")
            else:
                print(f"   âœ… å­¦ä¹ ç‡å·²è°ƒæ•´: {((lr_hist[-1] - lr_hist[0]) / lr_hist[0] * 100):.1f}%")
        else:
            print(f"\nğŸ“‰ å­¦ä¹ ç‡åˆ†æ:")
            print(f"   âš ï¸  æœªè®°å½•å­¦ä¹ ç‡å†å²ï¼Œå»ºè®®æ·»åŠ ReduceLROnPlateauå›è°ƒ")
        
        print(f"{'='*60}\n")
        
        final_ckpt = ckpt_h5.replace(
            '_best.weights.h5', f'_best-epoch{best_epoch:02d}-valacc{best_val:.4f}.weights.h5'
        )

    meta_path = ckpt_h5.replace('.weights.h5', '_params.json')
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump({
            "seed": seed,
            "best_epoch": best_epoch,
            "best_val_accuracy": best_val,
            "params": params
        }, f, ensure_ascii=False, indent=2)

print("å…¨éƒ¨å¾ªç¯ç»“æŸ")
